{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyterrier as pt\n",
    "import pandas as pd\n",
    "import os\n",
    "import csv\n",
    "import pandas as pd\n",
    "import pyterrier as pt\n",
    "from pyterrier.measures import *\n",
    "import numpy as np\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_all_runs(path, qrels_file, metric):\n",
    "    \"\"\"\n",
    "    Evaluates all TREC result files in a folder and returns per-query metric scores.\n",
    "\n",
    "    Parameters:\n",
    "    - results_folder (str): Path to folder containing .res files.\n",
    "    - qrels_file: Qrels file\n",
    "    - metric: pyterrier.measures metric (default: AP with rel >= 2).\n",
    "\n",
    "    Returns:\n",
    "    - dict: Nested dict {query_id: {model_name: score, ...}, ...}\n",
    "    \"\"\"\n",
    "    # Final nested dict: {qid: {model_name: score}}\n",
    "    all_results = {}\n",
    "    qrels = pt.io.read_qrels(qrels_file)\n",
    "\n",
    "    files = []\n",
    "    if os.path.isdir(path):\n",
    "        files = [os.path.join(path, f) for f in os.listdir(path) if f.endswith(\".res\")]\n",
    "    elif os.path.isfile(path) and path.endswith(\".res\"):\n",
    "        files = [path]\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid path: {path} is neither a .res file nor a directory containing .res files.\")\n",
    "    \n",
    "\n",
    "    for filename in files:            \n",
    "        model_name = os.path.basename(filename).replace(\".res\", \"\")\n",
    "\n",
    "        # Load run as DataFrame\n",
    "        run_df = pd.read_csv(filename, \n",
    "                     sep='\\\\s+', \n",
    "                     names=[\"qid\", \"iter\", \"docno\", \"rank\", \"score\", \"runid\"])\n",
    "\n",
    "        # Evaluate per query\n",
    "        perquery_results = pt.Evaluate(run_df, qrels, metrics=[metric], perquery=True)\n",
    "        perquery_df = pd.DataFrame.from_dict(perquery_results, orient='index')\n",
    "        \n",
    "        # Fill into nested dictionary\n",
    "        for qid, row in perquery_df.iterrows():\n",
    "            metric_name = perquery_df.columns[0]\n",
    "            score = row[metric_name]\n",
    "            all_results.setdefault(str(qid), {})[model_name] = score\n",
    "\n",
    "    return all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_qpp_estimates(path):\n",
    "    \"\"\"\n",
    "    Loads QPP estimates for all queries and IR models from a folder of .qpp files.\n",
    "\n",
    "    Each .qpp file is named after an IR model (e.g., 'bm25.qpp') and contains:\n",
    "    query_id \\t qpp1 \\t qpp2 \\t ... \\t qppN\n",
    "\n",
    "    Returns:\n",
    "    - dict of {query_id: {model_name: [qpp1, qpp2, ...]}}\n",
    "    \"\"\"\n",
    "    qpp_data = {}\n",
    "\n",
    "    files = []\n",
    "    if os.path.isdir(path):\n",
    "        files = [os.path.join(path, f) for f in os.listdir(path) if f.endswith(\".qpp\")]\n",
    "    elif os.path.isfile(path) and path.endswith(\".res\"):\n",
    "        files = [path+\".qpp\"]\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid path: {path} is neither a .res file nor a directory containing .res files.\")\n",
    "    \n",
    "    for filename in files:\n",
    "        model_name = os.path.basename(filename).replace(\".res.qpp\", \"\")        \n",
    "\n",
    "        with open(filename, \"r\") as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split(\"\\t\")\n",
    "                if len(parts) < 2:\n",
    "                    continue\n",
    "\n",
    "                qid = parts[0]\n",
    "                try:\n",
    "                    preds = [float(x) for x in parts[1:]]\n",
    "                except ValueError:\n",
    "                    continue  # skip malformed line\n",
    "\n",
    "                qpp_data.setdefault(qid, {})[model_name] = preds\n",
    "\n",
    "    return qpp_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import kendalltau\n",
    "\n",
    "def compute_perquery_kendall_multiqpp(qpp_estimates, true_ap_scores):\n",
    "    \"\"\"\n",
    "    Computes Kendall's tau correlation between each QPP model and true AP scores, per query.\n",
    "\n",
    "    Parameters:\n",
    "    - qpp_estimates: dict of {qid: {model: [qpp1, qpp2, ...]}}\n",
    "    - true_ap_scores: dict of {qid: {model: ap_score}}\n",
    "\n",
    "    Returns:\n",
    "    - dict of {qid: {qpp_model_index: tau_value}}\n",
    "    \"\"\"\n",
    "    perquery_tau = {}\n",
    "\n",
    "    for qid in qpp_estimates:\n",
    "        if qid not in true_ap_scores:\n",
    "            continue\n",
    "\n",
    "        models_in_common = set(qpp_estimates[qid]) & set(true_ap_scores[qid])\n",
    "        if len(models_in_common) < 2:\n",
    "            continue  # Not enough models to compute Kendall's tau\n",
    "\n",
    "        # For each QPP model index, collect predictions and corresponding AP scores\n",
    "        # We'll assume all QPP vectors are of the same length\n",
    "        qpp_len = len(next(iter(qpp_estimates[qid].values())))\n",
    "        perquery_tau[qid] = {}\n",
    "\n",
    "        for i in range(qpp_len):\n",
    "            qpp_vals = []\n",
    "            ap_vals = []\n",
    "\n",
    "            for model in sorted(models_in_common):\n",
    "                try:\n",
    "                    qpp_val = qpp_estimates[qid][model][i]\n",
    "                    ap_val = true_ap_scores[qid][model]\n",
    "                    qpp_vals.append(qpp_val)\n",
    "                    ap_vals.append(ap_val)\n",
    "                except (IndexError, KeyError):\n",
    "                    continue  # Skip any inconsistent entries\n",
    "\n",
    "            if len(qpp_vals) >= 2:\n",
    "                tau, _ = kendalltau(qpp_vals, ap_vals)\n",
    "                perquery_tau[qid][i] = tau\n",
    "\n",
    "    return perquery_tau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_modelwise_kendall_multiqpp(qpp_estimates, true_ap_scores):\n",
    "    \"\"\"\n",
    "    Computes Kendall's tau between QPP estimates and AP values across queries, \n",
    "    for each QPP model and IR model.\n",
    "\n",
    "    Parameters:\n",
    "    - qpp_estimates: dict of {qid: {model: [qpp1, qpp2, ...]}}\n",
    "    - true_ap_scores: dict of {qid: {model: ap_score}}\n",
    "\n",
    "    Returns:\n",
    "    - dict of {model: {qpp_model_index: tau_value}}\n",
    "    \"\"\"\n",
    "    modelwise_tau = {}\n",
    "\n",
    "    # Get all IR models (from union of inner keys)\n",
    "    all_models = set()\n",
    "    for qid in qpp_estimates:\n",
    "        all_models.update(qpp_estimates[qid].keys())\n",
    "    for qid in true_ap_scores:\n",
    "        all_models.update(true_ap_scores[qid].keys())\n",
    "\n",
    "    for model in sorted(all_models):\n",
    "        qpp_vals_by_index = defaultdict(list)\n",
    "        ap_vals = []\n",
    "\n",
    "        # For each query, collect the QPP predictions and AP for this model\n",
    "        for qid in qpp_estimates:\n",
    "            if model not in qpp_estimates[qid] or model not in true_ap_scores.get(qid, {}):\n",
    "                continue\n",
    "\n",
    "            qpp_preds = qpp_estimates[qid][model]\n",
    "            ap_val = true_ap_scores[qid][model]\n",
    "\n",
    "            for i, pred in enumerate(qpp_preds):\n",
    "                qpp_vals_by_index[i].append((qid, pred, ap_val))\n",
    "\n",
    "        modelwise_tau[model] = {}\n",
    "        for i, vals in qpp_vals_by_index.items():\n",
    "            qpp_list = [v[1] for v in vals]\n",
    "            ap_list = [v[2] for v in vals]\n",
    "\n",
    "            if len(qpp_list) >= 2:\n",
    "                tau, _ = kendalltau(qpp_list, ap_list)\n",
    "                modelwise_tau[model][i] = tau\n",
    "\n",
    "    return modelwise_tau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import kendalltau\n",
    "from collections import defaultdict\n",
    "\n",
    "def compute_global_kendall_multiqpp(qpp_estimates, true_ap_scores):\n",
    "    \"\"\"\n",
    "    Computes global Kendall's tau across all (query, model) pairs, for each QPP model.\n",
    "\n",
    "    Parameters:\n",
    "    - qpp_estimates: dict of {qid: {model: [qpp1, qpp2, ...]}}\n",
    "    - true_ap_scores: dict of {qid: {model: ap_score}}\n",
    "\n",
    "    Returns:\n",
    "    - dict of {qpp_model_index: global_kendall_tau}\n",
    "    \"\"\"\n",
    "    # Accumulate global lists of (QPP estimate, AP) for each QPP model\n",
    "    qpp_data = defaultdict(lambda: ([], []))  # index: (qpp_list, ap_list)\n",
    "\n",
    "    for qid in qpp_estimates:\n",
    "        if qid not in true_ap_scores:\n",
    "            continue\n",
    "        for model in qpp_estimates[qid]:\n",
    "            if model not in true_ap_scores[qid]:\n",
    "                continue\n",
    "\n",
    "            preds = qpp_estimates[qid][model]\n",
    "            ap = true_ap_scores[qid][model]\n",
    "\n",
    "            for i, pred in enumerate(preds):\n",
    "                qpp_data[i][0].append(pred)\n",
    "                qpp_data[i][1].append(ap)\n",
    "\n",
    "    # Compute Kendall's tau for each QPP model\n",
    "    global_tau = {}\n",
    "    for i in sorted(qpp_data.keys()):\n",
    "        qpp_vals, ap_vals = qpp_data[i]\n",
    "        if len(qpp_vals) >= 2:\n",
    "            tau, _ = kendalltau(qpp_vals, ap_vals)\n",
    "            global_tau[i] = tau\n",
    "\n",
    "    return global_tau\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaulate_all_qpp(qpp_estimates, eval_results):\n",
    "    tau_scores = compute_perquery_kendall_multiqpp(qpp_estimates, results)\n",
    "    qpp_model_avgs = defaultdict(list)\n",
    "    for qid in tau_scores:\n",
    "        for i, tau in tau_scores[qid].items():\n",
    "            if tau is not None:\n",
    "                qpp_model_avgs[i].append(tau)\n",
    "    \n",
    "    mean_local_taus = []\n",
    "    for i in sorted(qpp_model_avgs):\n",
    "        mean_tau = np.mean(qpp_model_avgs[i])\n",
    "        mean_local_taus.append(mean_tau)\n",
    "        print(f\"[Per-query QPP-eval (correlation across rankers averaged over queries)] QPP model {i}: mean Kendall's tau = {mean_tau:.4f}\")    \n",
    "\n",
    "    standard_tau = compute_modelwise_kendall_multiqpp(qpp_estimates, results)\n",
    "    \n",
    "    # Average Kendall's tau per QPP model (averaged over IR models)\n",
    "    qpp_model_avgs = defaultdict(list)\n",
    "    for model in standard_tau:\n",
    "        for i, tau in standard_tau[model].items():\n",
    "            if tau is not None:\n",
    "                qpp_model_avgs[i].append(tau)\n",
    "    \n",
    "    mean_taus = []\n",
    "    for i in sorted(qpp_model_avgs):\n",
    "        mean_tau = np.mean(qpp_model_avgs[i])\n",
    "        mean_taus.append(mean_tau)\n",
    "        print(f\"[Standard QPP-eval (correlation across queries averaged over rankers)] QPP model {i}: mean Kendall's tau = {mean_tau:.4f}\")\n",
    "    \n",
    "    global_tau = compute_global_kendall_multiqpp(qpp_estimates, results)\n",
    "    \n",
    "    for i in sorted(global_tau):\n",
    "        print(f\"[Global QPP-eval (correlation across each query-ranker pair)] QPP model {i}: Kendall's tau = {global_tau[i]:.4f}\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Standard QPP-eval (correlation across queries averaged over rankers)] QPP model 0: mean Kendall's tau = 0.2292\n",
      "[Standard QPP-eval (correlation across queries averaged over rankers)] QPP model 1: mean Kendall's tau = 0.2270\n",
      "[Standard QPP-eval (correlation across queries averaged over rankers)] QPP model 2: mean Kendall's tau = 0.1739\n",
      "[Global QPP-eval (correlation across each query-ranker pair)] QPP model 0: Kendall's tau = 0.2292\n",
      "[Global QPP-eval (correlation across each query-ranker pair)] QPP model 1: Kendall's tau = 0.2270\n",
      "[Global QPP-eval (correlation across each query-ranker pair)] QPP model 2: Kendall's tau = 0.1739\n"
     ]
    }
   ],
   "source": [
    "#respath=\"data/runs/2019\"\n",
    "#qrels_file=\"data/2019.qrels\"\n",
    "\n",
    "respath=\"data/runs/2019/BM25.2019.res\"\n",
    "qrels_file=\"data/2019.qrels\"\n",
    "\n",
    "results = evaluate_all_runs(respath, qrels_file, metric=AP(rel=2))\n",
    "qpp_estimates = load_qpp_estimates(respath)\n",
    "\n",
    "evaulate_all_qpp(qpp_estimates, results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
