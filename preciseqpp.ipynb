{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyterrier as pt\n",
    "import pandas as pd\n",
    "import os\n",
    "import csv\n",
    "import pandas as pd\n",
    "import pyterrier as pt\n",
    "from pyterrier.measures import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_all_runs(results_folder, qrels_file, metric):\n",
    "    \"\"\"\n",
    "    Evaluates all TREC result files in a folder and returns per-query metric scores.\n",
    "\n",
    "    Parameters:\n",
    "    - results_folder (str): Path to folder containing .res files.\n",
    "    - qrels_file: Qrels file\n",
    "    - metric: pyterrier.measures metric (default: AP with rel >= 2).\n",
    "\n",
    "    Returns:\n",
    "    - dict: Nested dict {query_id: {model_name: score, ...}, ...}\n",
    "    \"\"\"\n",
    "    # Final nested dict: {qid: {model_name: score}}\n",
    "    all_results = {}\n",
    "    qrels = pt.io.read_qrels(qrels_file)\n",
    "\n",
    "    for filename in os.listdir(results_folder):\n",
    "        filepath = os.path.join(results_folder, filename)\n",
    "        if not os.path.isfile(filepath) or not filename.endswith(\".res\"):\n",
    "            continue  # skip directories or non-res files\n",
    "            \n",
    "        model_name = os.path.splitext(filename)[0]  # remove .res extension\n",
    "\n",
    "        # Load run as DataFrame\n",
    "        run_df = pd.read_csv(filepath, \n",
    "                     sep='\\\\s+', \n",
    "                     names=[\"qid\", \"iter\", \"docno\", \"rank\", \"score\", \"runid\"])\n",
    "\n",
    "        # Evaluate per query\n",
    "        perquery_results = pt.Evaluate(run_df, qrels, metrics=[metric], perquery=True)\n",
    "        perquery_df = pd.DataFrame.from_dict(perquery_results, orient='index')\n",
    "        \n",
    "        # Fill into nested dictionary\n",
    "        for qid, row in perquery_df.iterrows():\n",
    "            metric_name = perquery_df.columns[0]\n",
    "            score = row[metric_name]\n",
    "            all_results.setdefault(str(qid), {})[model_name] = score\n",
    "\n",
    "    return all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        prf_rerank_beta05.2019  BM25.2019  prf_rank_beta05.2019  \\\n",
      "19335                 0.005410   0.417649              0.005223   \n",
      "47923                 0.269306   0.208209              0.270124   \n",
      "87181                 0.511408   0.224310              0.523762   \n",
      "87452                 0.130716   0.127828              0.133332   \n",
      "104861                0.544986   0.316859              0.546889   \n",
      "\n",
      "        prf_rerank_beta1.2019  prf_rank_beta1.2019  \n",
      "19335                0.005315             0.004859  \n",
      "47923                0.250536             0.250679  \n",
      "87181                0.480961             0.495223  \n",
      "87452                0.146770             0.151613  \n",
      "104861               0.621656             0.622021  \n",
      "0.005409924460404536\n"
     ]
    }
   ],
   "source": [
    "results = evaluate_all_runs(\"data/runs/2019\", qrels_file=\"data/2019.qrels\", metric=AP(rel=2))\n",
    "#print(results)\n",
    "\n",
    "# Convert to DataFrame\n",
    "results_df = pd.DataFrame.from_dict(results, orient='index')\n",
    "print(results_df.head())\n",
    "\n",
    "print(results[\"19335\"][\"prf_rerank_beta05.2019\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_qpp_estimates(folder):\n",
    "    \"\"\"\n",
    "    Loads QPP estimates for all queries and IR models from a folder of .qpp files.\n",
    "\n",
    "    Each .qpp file is named after an IR model (e.g., 'bm25.qpp') and contains:\n",
    "    query_id \\t qpp1 \\t qpp2 \\t ... \\t qppN\n",
    "\n",
    "    Returns:\n",
    "    - dict of {query_id: {model_name: [qpp1, qpp2, ...]}}\n",
    "    \"\"\"\n",
    "    qpp_data = {}\n",
    "\n",
    "    for filename in os.listdir(folder):\n",
    "        filepath = os.path.join(folder, filename)\n",
    "        if not os.path.isfile(filepath) or not filename.endswith(\".qpp\"):\n",
    "            continue\n",
    "        \n",
    "        model_name = os.path.splitext(filename)[0] # remove \".qpp\"\n",
    "        model_name = os.path.splitext(model_name)[0] # remove \".res\"\n",
    "        print (model_name)\n",
    "\n",
    "        with open(filepath, \"r\") as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split(\"\\t\")\n",
    "                if len(parts) < 2:\n",
    "                    continue\n",
    "\n",
    "                qid = parts[0]\n",
    "                try:\n",
    "                    preds = [float(x) for x in parts[1:]]\n",
    "                except ValueError:\n",
    "                    continue  # skip malformed line\n",
    "\n",
    "                qpp_data.setdefault(qid, {})[model_name] = preds\n",
    "\n",
    "    return qpp_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prf_rerank_beta1.2019\n",
      "prf_rerank_beta05.2019\n",
      "prf_rank_beta05.2019\n",
      "prf_rank_beta1.2019\n",
      "BM25.2019\n",
      "[7.1786523, 89.22529, 0.27864638]\n"
     ]
    }
   ],
   "source": [
    "qpp_estimates = load_qpp_estimates(\"data/runs/2019\")\n",
    "print(qpp_estimates[\"19335\"][\"prf_rerank_beta05.2019\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import kendalltau\n",
    "\n",
    "def compute_perquery_kendall_multiqpp(qpp_estimates, true_ap_scores):\n",
    "    \"\"\"\n",
    "    Computes Kendall's tau correlation between each QPP model and true AP scores, per query.\n",
    "\n",
    "    Parameters:\n",
    "    - qpp_estimates: dict of {qid: {model: [qpp1, qpp2, ...]}}\n",
    "    - true_ap_scores: dict of {qid: {model: ap_score}}\n",
    "\n",
    "    Returns:\n",
    "    - dict of {qid: {qpp_model_index: tau_value}}\n",
    "    \"\"\"\n",
    "    perquery_tau = {}\n",
    "\n",
    "    for qid in qpp_estimates:\n",
    "        if qid not in true_ap_scores:\n",
    "            continue\n",
    "\n",
    "        models_in_common = set(qpp_estimates[qid]) & set(true_ap_scores[qid])\n",
    "        if len(models_in_common) < 2:\n",
    "            continue  # Not enough models to compute Kendall's tau\n",
    "\n",
    "        # For each QPP model index, collect predictions and corresponding AP scores\n",
    "        # We'll assume all QPP vectors are of the same length\n",
    "        qpp_len = len(next(iter(qpp_estimates[qid].values())))\n",
    "        perquery_tau[qid] = {}\n",
    "\n",
    "        for i in range(qpp_len):\n",
    "            qpp_vals = []\n",
    "            ap_vals = []\n",
    "\n",
    "            for model in sorted(models_in_common):\n",
    "                try:\n",
    "                    qpp_val = qpp_estimates[qid][model][i]\n",
    "                    ap_val = true_ap_scores[qid][model]\n",
    "                    qpp_vals.append(qpp_val)\n",
    "                    ap_vals.append(ap_val)\n",
    "                except (IndexError, KeyError):\n",
    "                    continue  # Skip any inconsistent entries\n",
    "\n",
    "            if len(qpp_vals) >= 2:\n",
    "                tau, _ = kendalltau(qpp_vals, ap_vals)\n",
    "                perquery_tau[qid][i] = tau\n",
    "\n",
    "    return perquery_tau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tau_scores = compute_perquery_kendall_multiqpp(qpp_estimates, results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.31622776601683794\n"
     ]
    }
   ],
   "source": [
    "print (tau_scores['1037798'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QPP model 0: mean Kendall's tau = -0.0110\n",
      "QPP model 1: mean Kendall's tau = 0.0107\n",
      "QPP model 2: mean Kendall's tau = -0.1206\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "qpp_model_avgs = defaultdict(list)\n",
    "for qid in tau_scores:\n",
    "    for i, tau in tau_scores[qid].items():\n",
    "        if tau is not None:\n",
    "            qpp_model_avgs[i].append(tau)\n",
    "\n",
    "mean_local_taus = []\n",
    "for i in sorted(qpp_model_avgs):\n",
    "    mean_tau = np.mean(qpp_model_avgs[i])\n",
    "    mean_local_taus.append(mean_tau)\n",
    "    print(f\"QPP model {i}: mean Kendall's tau = {mean_tau:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_modelwise_kendall_multiqpp(qpp_estimates, true_ap_scores):\n",
    "    \"\"\"\n",
    "    Computes Kendall's tau between QPP estimates and AP values across queries, \n",
    "    for each QPP model and IR model.\n",
    "\n",
    "    Parameters:\n",
    "    - qpp_estimates: dict of {qid: {model: [qpp1, qpp2, ...]}}\n",
    "    - true_ap_scores: dict of {qid: {model: ap_score}}\n",
    "\n",
    "    Returns:\n",
    "    - dict of {model: {qpp_model_index: tau_value}}\n",
    "    \"\"\"\n",
    "    modelwise_tau = {}\n",
    "\n",
    "    # Get all IR models (from union of inner keys)\n",
    "    all_models = set()\n",
    "    for qid in qpp_estimates:\n",
    "        all_models.update(qpp_estimates[qid].keys())\n",
    "    for qid in true_ap_scores:\n",
    "        all_models.update(true_ap_scores[qid].keys())\n",
    "\n",
    "    for model in sorted(all_models):\n",
    "        qpp_vals_by_index = defaultdict(list)\n",
    "        ap_vals = []\n",
    "\n",
    "        # For each query, collect the QPP predictions and AP for this model\n",
    "        for qid in qpp_estimates:\n",
    "            if model not in qpp_estimates[qid] or model not in true_ap_scores.get(qid, {}):\n",
    "                continue\n",
    "\n",
    "            qpp_preds = qpp_estimates[qid][model]\n",
    "            ap_val = true_ap_scores[qid][model]\n",
    "\n",
    "            for i, pred in enumerate(qpp_preds):\n",
    "                qpp_vals_by_index[i].append((qid, pred, ap_val))\n",
    "\n",
    "        modelwise_tau[model] = {}\n",
    "        for i, vals in qpp_vals_by_index.items():\n",
    "            qpp_list = [v[1] for v in vals]\n",
    "            ap_list = [v[2] for v in vals]\n",
    "\n",
    "            if len(qpp_list) >= 2:\n",
    "                tau, _ = kendalltau(qpp_list, ap_list)\n",
    "                modelwise_tau[model][i] = tau\n",
    "\n",
    "    return modelwise_tau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Standard QPP Eval] QPP model 0: mean Kendall's tau = 0.3059\n",
      "[Standard QPP Eval] QPP model 1: mean Kendall's tau = 0.3054\n",
      "[Standard QPP Eval] QPP model 2: mean Kendall's tau = 0.1070\n"
     ]
    }
   ],
   "source": [
    "standard_tau = compute_modelwise_kendall_multiqpp(qpp_estimates, results)\n",
    "\n",
    "# Average Kendall's tau per QPP model (averaged over IR models)\n",
    "qpp_model_avgs = defaultdict(list)\n",
    "for model in standard_tau:\n",
    "    for i, tau in standard_tau[model].items():\n",
    "        if tau is not None:\n",
    "            qpp_model_avgs[i].append(tau)\n",
    "\n",
    "mean_taus = []\n",
    "for i in sorted(qpp_model_avgs):\n",
    "    mean_tau = np.mean(qpp_model_avgs[i])\n",
    "    mean_taus.append(mean_tau)\n",
    "    print(f\"[Standard QPP Eval] QPP model {i}: mean Kendall's tau = {mean_tau:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.33333333333333337\n"
     ]
    }
   ],
   "source": [
    "qpp_eval_agreement, _ = kendalltau(mean_local_taus, mean_taus)\n",
    "print(qpp_eval_agreement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
